{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5490c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5724bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e2fba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f971d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfeafa8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286770a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TALC paths\n",
    "BASE_DIR = \"/work/TALC/ensf617_2026w/garbage_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e8891",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join(BASE_DIR, \"CVPR_2024_dataset_Train\")\n",
    "VAL_DIR   = os.path.join(BASE_DIR, \"CVPR_2024_dataset_Val\")\n",
    "TEST_DIR  = os.path.join(BASE_DIR, \"CVPR_2024_dataset_Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always align class order to folder names\n",
    "CLASS_NAMES = sorted([d for d in os.listdir(TRAIN_DIR)\n",
    "                      if os.path.isdir(os.path.join(TRAIN_DIR, d))])\n",
    "print(\"CLASS_NAMES:\", CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    print(d, \"exists:\", os.path.exists(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a74e94",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Quick count\n",
    "def count_images(root_dir):\n",
    "    total = 0\n",
    "    for cls in CLASS_NAMES:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        if not os.path.exists(cls_dir):\n",
    "            continue\n",
    "        total += len([f for f in os.listdir(cls_dir)\n",
    "                      if f.lower().endswith((\".jpg\",\".jpeg\",\".png\"))])\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN:\", count_images(TRAIN_DIR))\n",
    "print(\"VAL  :\", count_images(VAL_DIR))\n",
    "print(\"TEST :\", count_images(TEST_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b8f2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Text processing + vocab\n",
    "def filename_to_text(fname: str) -> str:\n",
    "    base = os.path.splitext(fname)[0]\n",
    "    base = re.sub(r\"_\\d+$\", \"\", base)\n",
    "    return base.replace(\"_\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f08c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize(text: str):\n",
    "    return re.findall(r\"[a-zA-Z]+\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501bf17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_vocab_from_dirs(dirs, class_names, max_vocab=5000, min_freq=2):\n",
    "    counter = Counter()\n",
    "    for root in dirs:\n",
    "        for cls in class_names:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            if not os.path.exists(cls_dir):\n",
    "                continue\n",
    "            for f in os.listdir(cls_dir):\n",
    "                if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
    "                    counter.update(tokenize(filename_to_text(f)))\n",
    "\n",
    "    vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for word, freq in counter.most_common():\n",
    "        if freq >= min_freq and len(vocab) < max_vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc25223",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = build_vocab_from_dirs([TRAIN_DIR, VAL_DIR], CLASS_NAMES)\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "print(\"Vocab size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b746cf2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dataset (image + text_vec)\n",
    "class ImageTextGarbageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, vocab=None, class_names=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "        self.class_names = class_names\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "        self.samples = []\n",
    "        for cls in class_names:\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.exists(cls_dir):\n",
    "                continue\n",
    "            for f in os.listdir(cls_dir):\n",
    "                if f.lower().endswith((\".jpg\",\".jpeg\",\".png\")):\n",
    "                    self.samples.append((\n",
    "                        os.path.join(cls_dir, f),\n",
    "                        filename_to_text(f),\n",
    "                        self.class_to_idx[cls]\n",
    "                    ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def encode_text_bow(self, text):\n",
    "        vec = torch.zeros(len(self.vocab), dtype=torch.float32)\n",
    "        for w in tokenize(text):\n",
    "            vec[self.vocab.get(w, self.vocab[\"<unk>\"])] += 1.0\n",
    "        if vec.sum() > 0:\n",
    "            vec /= vec.sum()\n",
    "        return vec\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, text, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"text_vec\": self.encode_text_bow(text),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"path\": path,\n",
    "            \"text\": text\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets + Dataloaders\n",
    "datasets = {\n",
    "    \"train\": ImageTextGarbageDataset(TRAIN_DIR, transform[\"train\"], VOCAB, CLASS_NAMES),\n",
    "    \"val\":   ImageTextGarbageDataset(VAL_DIR,   transform[\"val\"],   VOCAB, CLASS_NAMES),\n",
    "    \"test\":  ImageTextGarbageDataset(TEST_DIR,  transform[\"test\"],  VOCAB, CLASS_NAMES),\n",
    "}\n",
    "print(\"Dataset sizes:\", {k: len(v) for k, v in datasets.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da723ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = device.type == \"cuda\"\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(datasets[\"train\"], batch_size=32, shuffle=True,  num_workers=0, pin_memory=pin),\n",
    "    \"val\":   DataLoader(datasets[\"val\"],   batch_size=32, shuffle=False, num_workers=0, pin_memory=pin),\n",
    "    \"test\":  DataLoader(datasets[\"test\"],  batch_size=32, shuffle=False, num_workers=0, pin_memory=pin),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e98a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (ResNet50 + text MLP)\n",
    "class ResNetMultimodalClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes):\n",
    "        super().__init__()\n",
    "        try:\n",
    "            resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        except AttributeError:\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        self.image_features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.image_fc = nn.Linear(2048, 512)\n",
    "\n",
    "        self.text_fc = nn.Sequential(\n",
    "            nn.Linear(vocab_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, images, text_vec):\n",
    "        img = self.image_features(images).squeeze(-1).squeeze(-1)\n",
    "        img = self.image_fc(img)\n",
    "        txt = self.text_fc(text_vec)\n",
    "        fused = torch.cat((img, txt), dim=1)\n",
    "        return self.classifier(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c872b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, loaders, criterion, optimizer, epochs, device, save_path):\n",
    "    best_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        print(f\"\\nEpoch {ep+1}/{epochs}\")\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            loss_sum = 0.0\n",
    "            correct = 0\n",
    "\n",
    "            for batch in tqdm(loaders[phase], leave=False):\n",
    "                imgs = batch[\"image\"].to(device)\n",
    "                txts = batch[\"text_vec\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(imgs, txts)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                loss_sum += loss.item() * imgs.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            epoch_loss = loss_sum / len(loaders[phase].dataset)\n",
    "            epoch_acc = correct / len(loaders[phase].dataset)\n",
    "\n",
    "            history[f\"{phase}_loss\"].append(epoch_loss)\n",
    "            history[f\"{phase}_acc\"].append(epoch_acc)\n",
    "\n",
    "            print(f\"{phase}: loss={epoch_loss:.4f}, acc={epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\" Saved best model: {save_path} (val acc={best_acc:.4f})\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95cf6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = ResNetMultimodalClassifier(VOCAB_SIZE, len(CLASS_NAMES)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.path.join(BASE_DIR, \"outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf76d52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(OUT_DIR, \"best_model.pth\")\n",
    "# Resume if checkpoint exists\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\" Resuming from:\", MODEL_PATH)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "history = train_model(model, dataloaders, criterion, optimizer, epochs=8,\n",
    "                      device=device, save_path=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save curves\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.savefig(os.path.join(OUT_DIR, \"loss_curve.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52713ea1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs Epoch\")\n",
    "plt.savefig(os.path.join(OUT_DIR, \"acc_curve.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation\n",
    "print(\"\\nLoading best model from:\", MODEL_PATH)\n",
    "print(\"Exists:\", os.path.exists(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetMultimodalClassifier(vocab_size=VOCAB_SIZE, num_classes=len(CLASS_NAMES)).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = dataloaders[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels, all_paths, all_texts = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90af425",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        text_vec = batch[\"text_vec\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(images, text_vec)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_paths.extend(batch[\"path\"])\n",
    "        all_texts.extend(batch[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "print(f\"\\nAccuracy on test set: {accuracy:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6de044",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(\"Confusion Matrix (Test)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2e50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "for name, acc in zip(CLASS_NAMES, per_class_accuracy):\n",
    "    print(f\"Accuracy for {name}: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db1cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misclassified examples\n",
    "misclassified = {name: [] for name in CLASS_NAMES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (y, p) in enumerate(zip(all_labels, all_preds)):\n",
    "    if y != p:\n",
    "        true_name = CLASS_NAMES[y]\n",
    "        pred_name = CLASS_NAMES[p]\n",
    "\n",
    "        img = Image.open(all_paths[i]).convert(\"RGB\")\n",
    "        img = transform[\"test\"](img).cpu().numpy().transpose(1,2,0)\n",
    "        img = (img * std) + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        misclassified[true_name].append({\n",
    "            \"image\": img,\n",
    "            \"true\": true_name,\n",
    "            \"pred\": pred_name,\n",
    "            \"text\": all_texts[i]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 12))\n",
    "rows = len(CLASS_NAMES)\n",
    "for row, cname in enumerate(CLASS_NAMES):\n",
    "    examples = misclassified[cname]\n",
    "    if len(examples) == 0:\n",
    "        continue\n",
    "    selected = random.sample(examples, min(3, len(examples)))\n",
    "    for col, ex in enumerate(selected):\n",
    "        plt.subplot(rows, 3, row*3 + col + 1)\n",
    "        plt.imshow(ex[\"image\"])\n",
    "        plt.title(f\"True: {ex['true']}\\nPred: {ex['pred']}\\n{ex['text'][:20]}\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"misclassified_examples.png\"), dpi=200, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0481db",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "for cname in CLASS_NAMES:\n",
    "    print(f\"{cname}: {len(misclassified[cname])} misclassified examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8913f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"path\": all_paths,\n",
    "    \"text\": all_texts,\n",
    "    \"true\": [CLASS_NAMES[i] for i in all_labels],\n",
    "    \"pred\": [CLASS_NAMES[i] for i in all_preds],\n",
    "})\n",
    "csv_path = os.path.join(OUT_DIR, \"test_predictions.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(\"Saved predictions CSV:\", csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e33ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"\\n DONE. Outputs saved to:\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
